{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "避免过拟合。\n",
    "\n",
    "> 如何避免过拟合\n",
    "\n",
    "- 早期停止\n",
    "- L1和L2正则\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "# 使用gpu\n",
    "# (useful when running multiple experiments in parallel, on different GPUs):\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# 随机种子:\n",
    "random_seed = 42\n",
    "\n",
    "# 定制日志格式:\n",
    "log_begin_red, log_begin_blue, log_begin_green = '\\033[91m', '\\033[94m', '\\033[92m'\n",
    "log_begin_bold, log_begin_underline = '\\033[1m', '\\033[4m'\n",
    "log_end_format = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据\n",
    "\n",
    "我们再次使用 [MNIST](http://yann.lecun.com/exdb/mnist) 数据[$^1$](#ref) 作为演示。\n",
    "因此，我们按照之前笔记本中的方法准备数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "img_rows, img_cols, img_ch = 28, 28, 1\n",
    "input_shape = (img_rows, img_cols, img_ch)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *input_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一次，为了突出正则化的优势，我们将通过人为 __减少可用于训练集数量__ 来增加识别任务的难度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (200, 28, 28, 1)\n",
      "Testing data: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# ... 200 training samples instead of 60,000...\n",
    "x_train, y_train = x_train[:200], y_train[:200]\n",
    "\n",
    "print('Training data: {}'.format(x_train.shape))\n",
    "print('Testing data: {}'.format(x_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练一个带正则化的模型\n",
    "\n",
    "根据本章介绍的代码，我们将首先演示如何实施和应用规则化。\n",
    "\n",
    "然后，我们将展示如何直接使用Keras API使用标准正则化解决方案来训练模型 (*L1/L2*, *失活*, *批量标准*), \n",
    "用于比较效果.   \n",
    "\n",
    "我们将使用 *LeNet-5*[$^2$](#ref) 和 MNIST 举例说明."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Conv2D,\n",
    "                                     MaxPooling2D, Dropout, BatchNormalization)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动处理正则化损失\n",
    "\n",
    "为了演示如何将正则化损耗添加到任何层，我们将从我们在本书和之前的[note](./note2_实现第一个CNN.ipynb)中介绍的简单卷积层开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def conv_layer(x, kernels, bias, s):\n",
    "    z = tf.nn.conv2d(x, kernels, strides=[1, s, s, 1], padding='VALID')\n",
    "    # Finally, applying the bias and activation function (e.g. ReLU):\n",
    "    return tf.nn.relu(z + bias)\n",
    "\n",
    "class SimpleConvolutionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1):\n",
    "        \"\"\"\n",
    "        初始化 layer.\n",
    "        :param num_kernels:     卷积核数量\n",
    "        :param kernel_size:     核尺寸 (H x W)\n",
    "        :param stride:          步长\n",
    "        \"\"\"\n",
    "        # Then we assign the parameters:\n",
    "        super().__init__()\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        构建 layer, 根据输入形状初始化其参数.\n",
    "        不过，第一次使用该层时，将在内部调用该函数，它也可以手动调用.\n",
    "        :param input_shape: 输入图层将接收的形状(e.g. B x H x W x C)\n",
    "        \"\"\"\n",
    "\n",
    "        #  获取 通道数量:\n",
    "        num_input_ch = input_shape[-1]\n",
    "\n",
    "       # 重新调整核的形状\n",
    "        kernels_shape = (*self.kernel_size, num_input_ch, self.num_kernels)\n",
    "\n",
    "        # 我们使用从Glorot分布中选取的值初始化过滤器:\n",
    "        glorot_init = tf.initializers.GlorotUniform()\n",
    "\n",
    "        self.kernels = self.add_weight(\n",
    "            name='kernels', shape=kernels_shape, initializer=glorot_init,\n",
    "            trainable=True)  # 可训练的变量\n",
    "\n",
    "        # 使用B:\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias', shape=(self.num_kernels,),\n",
    "            initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        调用层并对输入张量执行其操作\n",
    "        :param inputs:  Input tensor\n",
    "        :return:        Output tensor\n",
    "        \"\"\"\n",
    "        return conv_layer(inputs, self.kernels, self.bias, self.stride)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        辅助函数返回定义的层和参数信息.\n",
    "        :return:        Dictionary containing the layer's configuration\n",
    "        \"\"\"\n",
    "        return {'num_kernels': self.num_kernels,\n",
    "                'kernel_size': self.kernel_size,\n",
    "                'strides': self.strides,\n",
    "                'use_bias': self.use_bias}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将扩展这个layer类来添加  内核/偏置数 正则化。 \n",
    "\n",
    "书中所示，使用`Layer.add_loss()`实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def l2_reg(coef=1e-2):\n",
    "    \"\"\"\n",
    "    返回一个函数，该函数计算给定张量的加权L2范数.\n",
    "    (this is basically a reimplementation of f.keras.regularizers.l2())\n",
    "    :param coef:    系数-标准权重\n",
    "    :return:        Loss function (损失函数)\n",
    "    \"\"\"\n",
    "    return lambda x: tf.reduce_sum(x ** 2) * coef\n",
    "\n",
    "\n",
    "class ConvWithRegularizers(SimpleConvolutionLayer):\n",
    "    \"\"\"\n",
    "    带正则的卷积层\n",
    "    \"\"\"\n",
    "    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1,\n",
    "                 kernel_regularizer=l2_reg(), bias_regularizer=None):\n",
    "        \"\"\" \n",
    "        Initialize the layer.\n",
    "        :param num_kernels:        卷积核的数量\n",
    "        :param kernel_size:        核尺寸 (H x W)\n",
    "        :param stride:             Vertical/horizontal 步长\n",
    "        :param kernel_regularizer: (opt.) 核的损失函数\n",
    "        :param bias_regularizer:   (opt.) bias 的损失函数\n",
    "        \"\"\"\n",
    "        super().__init__(num_kernels, kernel_size, stride)\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        构建 layer, 初始化其组件.\n",
    "        \"\"\"\n",
    "        super().build(input_shape)\n",
    "        # Attaching the regularization losses to the variables.\n",
    "        if self.kernel_regularizer is not None:\n",
    "            self.add_loss(partial(self.kernel_regularizer, self.kernels))\n",
    "        if self.bias_regularizer is not None:\n",
    "            self.add_loss(partial(self.bias_regularizer, self.bias))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化这一层，正则化器 将 作为属性传递到这一层，   \n",
    "无论何时，都可以获得这些正则化器的损失值，只需调用层的属性 `.losses`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "核参数和偏差参数的正则化损失: [1.9108772, 0.07420379]\n",
      "核参数和偏差参数的L2参数: [1.9108772, 0.07420379]\n"
     ]
    }
   ],
   "source": [
    "conv = ConvWithRegularizers(num_kernels=32, kernel_size=(3, 3), stride=1,\n",
    "                            kernel_regularizer=l2_reg(1.), bias_regularizer=l2_reg(1.))\n",
    "\n",
    "conv.build(input_shape=tf.TensorShape((None, 28, 28, 1)))\n",
    "\n",
    "# Fetching the layer's losses:\n",
    "reg_losses = conv.losses\n",
    "print('核参数和偏差参数的正则化损失: {}'.format(\n",
    "    [loss.numpy() for loss in reg_losses]))\n",
    "\n",
    "# 与核张量和偏张量的L2范数比较:\n",
    "kernel_norm, bias_norm = tf.reduce_sum(\n",
    "    conv.kernels ** 2).numpy(), tf.reduce_sum(conv.bias ** 2).numpy()\n",
    "print('核参数和偏差参数的L2参数: {}'.format(\n",
    "    [kernel_norm, bias_norm]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neat thing with the property `.losses` is that it also list the losses attached to all the layers and  models composing an instance. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses 函数附加到模型以及他们的layers :\n",
      "[2.0546362, 0.06137651, 32.179474, 0.07327281, 32.16109, 0.105349846] (6 losses)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    ConvWithRegularizers(kernel_regularizer=l2_reg(1.),\n",
    "                         bias_regularizer=l2_reg(1.)),\n",
    "    ConvWithRegularizers(kernel_regularizer=l2_reg(1.),\n",
    "                         bias_regularizer=l2_reg(1.)),\n",
    "    ConvWithRegularizers(kernel_regularizer=l2_reg(1.),\n",
    "                         bias_regularizer=l2_reg(1.))\n",
    "])\n",
    "\n",
    "print('Losses 函数附加到模型以及他们的layers :\\n\\r{} ({} losses)'.format(\n",
    "    [loss.numpy() for loss in model.losses], len(model.losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Model):  # `Model` has the same API as `Layer` + extends it\n",
    "\n",
    "    def __init__(self, num_classes,\n",
    "                 kernel_regularizer=l2_reg(), bias_regularizer=l2_reg()):\n",
    "        # Create the model and its layers:\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = ConvWithRegularizers(\n",
    "            6, kernel_size=(5, 5),\n",
    "            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)\n",
    "        self.conv2 = ConvWithRegularizers(\n",
    "            16, kernel_size=(5, 5),\n",
    "            kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer)\n",
    "        self.max_pool = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(120, activation='relu')\n",
    "        self.dense2 = Dense(84, activation='relu')\n",
    "        self.dense3 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x):  # Apply the layers in order to process the inputs\n",
    "        x = self.max_pool(self.conv1(x))  # 1st block\n",
    "        x = self.max_pool(self.conv2(x))  # 2nd block\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense3(self.dense2(self.dense1(x)))  # dense layers\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \u001b[91mstart\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function conv_layer at 0x181f862f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch   0/200: main loss = \u001b[94m2.260\u001b[0m; reg loss = \u001b[94m0.106\u001b[0m; val acc = \u001b[94m25.310%\u001b[0m\n",
      "Epoch  10/200: main loss = \u001b[94m0.805\u001b[0m; reg loss = \u001b[94m0.151\u001b[0m; val acc = \u001b[94m69.140%\u001b[0m\n",
      "Epoch  20/200: main loss = \u001b[94m0.007\u001b[0m; reg loss = \u001b[94m0.103\u001b[0m; val acc = \u001b[94m82.550%\u001b[0m\n",
      "Epoch  30/200: main loss = \u001b[94m0.006\u001b[0m; reg loss = \u001b[94m0.063\u001b[0m; val acc = \u001b[94m82.940%\u001b[0m\n",
      "Epoch  40/200: main loss = \u001b[94m0.006\u001b[0m; reg loss = \u001b[94m0.045\u001b[0m; val acc = \u001b[94m83.110%\u001b[0m\n",
      "Epoch  50/200: main loss = \u001b[94m0.006\u001b[0m; reg loss = \u001b[94m0.036\u001b[0m; val acc = \u001b[94m83.400%\u001b[0m\n",
      "Epoch  60/200: main loss = \u001b[94m0.005\u001b[0m; reg loss = \u001b[94m0.031\u001b[0m; val acc = \u001b[94m83.600%\u001b[0m\n",
      "Epoch  70/200: main loss = \u001b[94m0.005\u001b[0m; reg loss = \u001b[94m0.028\u001b[0m; val acc = \u001b[94m83.580%\u001b[0m\n",
      "Epoch  80/200: main loss = \u001b[94m0.004\u001b[0m; reg loss = \u001b[94m0.025\u001b[0m; val acc = \u001b[94m83.680%\u001b[0m\n",
      "Epoch  90/200: main loss = \u001b[94m0.004\u001b[0m; reg loss = \u001b[94m0.023\u001b[0m; val acc = \u001b[94m83.790%\u001b[0m\n",
      "Epoch 100/200: main loss = \u001b[94m0.004\u001b[0m; reg loss = \u001b[94m0.022\u001b[0m; val acc = \u001b[94m83.810%\u001b[0m\n",
      "Epoch 110/200: main loss = \u001b[94m0.004\u001b[0m; reg loss = \u001b[94m0.021\u001b[0m; val acc = \u001b[94m83.780%\u001b[0m\n",
      "Epoch 120/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.020\u001b[0m; val acc = \u001b[94m83.890%\u001b[0m\n",
      "Epoch 130/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.019\u001b[0m; val acc = \u001b[94m83.810%\u001b[0m\n",
      "Epoch 140/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.018\u001b[0m; val acc = \u001b[94m83.850%\u001b[0m\n",
      "Epoch 150/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.017\u001b[0m; val acc = \u001b[94m83.900%\u001b[0m\n",
      "Epoch 160/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.017\u001b[0m; val acc = \u001b[94m83.920%\u001b[0m\n",
      "Epoch 170/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.016\u001b[0m; val acc = \u001b[94m83.960%\u001b[0m\n",
      "Epoch 180/200: main loss = \u001b[94m0.003\u001b[0m; reg loss = \u001b[94m0.016\u001b[0m; val acc = \u001b[94m83.980%\u001b[0m\n",
      "Epoch 190/200: main loss = \u001b[94m0.002\u001b[0m; reg loss = \u001b[94m0.015\u001b[0m; val acc = \u001b[94m84.030%\u001b[0m\n",
      "Epoch 199/200: main loss = \u001b[94m0.002\u001b[0m; reg loss = \u001b[94m0.015\u001b[0m; val acc = \u001b[94m83.980%\u001b[0m\n",
      "Training: \u001b[92mend\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.SGD()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(batch_size)\n",
    "log_string_template = 'Epoch {0:3}/{1}: main loss = {5}{2:5.3f}{6}; ' + \\\n",
    "                      'reg loss = {5}{3:5.3f}{6}; val acc = {5}{4:5.3f}%{6}'\n",
    "\n",
    "\n",
    "def train_classifier_on_mnist(model, log_frequency=10):\n",
    "\n",
    "    avg_main_loss = tf.keras.metrics.Mean(\n",
    "        name='avg_main_loss', dtype=tf.float32)\n",
    "    avg_reg_loss = tf.keras.metrics.Mean(name='avg_reg_loss', dtype=tf.float32)\n",
    "\n",
    "    print(\"Training: {}start{}\".format(log_begin_red, log_end_format))\n",
    "    for epoch in range(epochs):\n",
    "        for (batch_images, batch_gts) in dataset:    # For each batch of this epoch\n",
    "\n",
    "            with tf.GradientTape() as grad_tape:     # Tell TF to tape the gradients\n",
    "                y = model(batch_images)              # Feed forward\n",
    "                main_loss = tf.losses.sparse_categorical_crossentropy(\n",
    "                    batch_gts, y)                    # Compute loss\n",
    "                # List and add other losses\n",
    "                reg_loss = sum(model.losses)\n",
    "                loss = main_loss + reg_loss\n",
    "\n",
    "            # Get the gradients of combined losses and back-propagate:\n",
    "            grads = grad_tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Keep track of losses for display:\n",
    "            avg_main_loss.update_state(main_loss)\n",
    "            avg_reg_loss.update_state(reg_loss)\n",
    "\n",
    "        # Log some metrics\n",
    "        if epoch % log_frequency == 0 or epoch == (epochs - 1):\n",
    "            # Validate, computing the accuracy on test data:\n",
    "            acc = tf.reduce_mean(tf.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_test), model(x_test))).numpy() * 100\n",
    "\n",
    "            main_loss = avg_main_loss.result()\n",
    "            reg_loss = avg_reg_loss.result()\n",
    "\n",
    "            print(log_string_template.format(\n",
    "                epoch, epochs, main_loss, reg_loss, acc, log_begin_blue, log_end_format))\n",
    "\n",
    "        avg_main_loss.reset_states()\n",
    "        avg_reg_loss.reset_states()\n",
    "    print(\"Training: {}end{}\".format(log_begin_green, log_end_format))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = LeNet5(10, kernel_regularizer=l2_reg(), bias_regularizer=l2_reg())\n",
    "model = train_classifier_on_mnist(model, log_frequency=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有趣的是，首先，正则化损失增加，而分类损失减少。\n",
    "由于后者的值一开始要高得多，网络基本上专注于最小化它，而不管它的内核/偏差值是多少。\n",
    "一旦分类下降到足够低的水平，那么规则化损失也开始被考虑在内。\n",
    "\n",
    "让我们将正则化网络的精度与没有这些项的网络进行比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \u001b[91mstart\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function conv_layer at 0x181f862f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch   0/200: main loss = \u001b[94m2.250\u001b[0m; reg loss = \u001b[94m0.000\u001b[0m; val acc = \u001b[94m23.310%\u001b[0m\n",
      "Epoch  50/200: main loss = \u001b[94m0.000\u001b[0m; reg loss = \u001b[94m0.000\u001b[0m; val acc = \u001b[94m83.120%\u001b[0m\n",
      "Epoch 100/200: main loss = \u001b[94m0.000\u001b[0m; reg loss = \u001b[94m0.000\u001b[0m; val acc = \u001b[94m83.150%\u001b[0m\n",
      "Epoch 150/200: main loss = \u001b[94m0.000\u001b[0m; reg loss = \u001b[94m0.000\u001b[0m; val acc = \u001b[94m83.250%\u001b[0m\n",
      "Epoch 199/200: main loss = \u001b[94m0.000\u001b[0m; reg loss = \u001b[94m0.000\u001b[0m; val acc = \u001b[94m83.330%\u001b[0m\n",
      "Training: \u001b[92mend\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5(10, kernel_regularizer=None, bias_regularizer=None)\n",
    "model = train_classifier_on_mnist(model, log_frequency=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集的精度提高了很少%，这是不容忽视的！\n",
    "\n",
    "使用 **`add_loss()`** 和 **`.losses`**  方法， 损失是这个实验的主要差异 , 因为它们可以用于更复杂的模型, 例如，当我们想要应用特定于层的损耗时.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用各种预先实现的正则化方法\n",
    "\n",
    "除了L1/L2正则化，本章还介绍了其他方法\n",
    "\n",
    "\n",
    "完全切换到KerasAPI，我们将试验这些方法，并快速比较它们对我们的玩具用例的影响。\n",
    "\n",
    "\n",
    "为此，让我们创建另一个_LeNet-5_工厂函数（这次使用顺序API。只是为了说明区别）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(name='lenet', input_shape=input_shape,\n",
    "          use_dropout=False, use_batchnorm=False, regularizer=None):\n",
    "    \"\"\"\n",
    "    Create a LeNet-5 Keras model, with optional regularization schemes.\n",
    "    :param name:           Name for the model\n",
    "    :param input_shape:    Model's input shape\n",
    "    :param use_dropout:    Flag to add Dropout layers after key layers\n",
    "    :param use_batchnorm:  Flag to add BatchNormalization layers after key layers\n",
    "    :param regularizer:    Regularization function to be applied to layers' kernels\n",
    "    :return:               LeNet-5 Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # 1st block:\n",
    "    layers += [Conv2D(6, kernel_size=(5, 5), padding='same',\n",
    "                      input_shape=input_shape, kernel_regularizer=regularizer)]\n",
    "    if use_batchnorm:\n",
    "        layers += [BatchNormalization()]\n",
    "    layers += [Activation('relu'),\n",
    "               MaxPooling2D(pool_size=(2, 2))]\n",
    "    if use_dropout:\n",
    "        layers += [Dropout(0.25)]\n",
    "\n",
    "    # 2nd block:\n",
    "    layers += [\n",
    "        Conv2D(16, kernel_size=(5, 5), kernel_regularizer=regularizer)]\n",
    "    if use_batchnorm:\n",
    "        layers += [BatchNormalization()]\n",
    "    layers += [Activation('relu'),\n",
    "               MaxPooling2D(pool_size=(2, 2))]\n",
    "    if use_dropout:\n",
    "        layers += [Dropout(0.25)]\n",
    "\n",
    "    # Dense layers:\n",
    "    layers += [Flatten()]\n",
    "\n",
    "    layers += [Dense(120, kernel_regularizer=regularizer)]\n",
    "    if use_batchnorm:\n",
    "        layers += [BatchNormalization()]\n",
    "    layers += [Activation('relu')]\n",
    "    if use_dropout:\n",
    "        layers += [Dropout(0.25)]\n",
    "\n",
    "    layers += [Dense(84, kernel_regularizer=regularizer)]\n",
    "    layers += [Activation('relu')]\n",
    "\n",
    "    layers += [Dense(num_classes, activation='softmax')]\n",
    "\n",
    "    model = Sequential(layers, name=name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了展示流星的 优化器（Tensorflow和Keras中提供）对训练的影响，\n",
    "\n",
    "我们将创建几个类似的LeNet实例，并使用不同的正则化技术组合[$^{3,4,5}$]（#ref）对每个实例进行培训。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    'none':         {'use_dropout': False, 'use_batchnorm': False, 'regularizer': None},\n",
    "    'l1':           {'use_dropout': False, 'use_batchnorm': False, 'regularizer': tf.keras.regularizers.l1(0.01)},\n",
    "    'l2':           {'use_dropout': False, 'use_batchnorm': False, 'regularizer': tf.keras.regularizers.l2(0.01)},\n",
    "    'dropout':      {'use_dropout': True,  'use_batchnorm': False, 'regularizer': None},\n",
    "    'bn':           {'use_dropout': False, 'use_batchnorm': True,  'regularizer': None},\n",
    "    # 'l1+dropout':   {'use_dropout': False, 'use_batchnorm': True,  'regularizer': tf.keras.regularizers.l1(0.01)},\n",
    "    'l1+bn':        {'use_dropout': False, 'use_batchnorm': True,  'regularizer': tf.keras.regularizers.l1(0.01)},\n",
    "    'l1+dropout+bn': {'use_dropout': False, 'use_batchnorm': True,  'regularizer': tf.keras.regularizers.l1(0.01)}\n",
    "    # ...\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于我们正在考虑的每个正则化配置，\n",
    "我们将实例化一个新的LeNet模型并使用它进行训练。我们将保存他们的训练“历史记录”（包含续联期间的损失和指标历史记录），以供比较*（此过程需要时间，尤其是在CPU上！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: \u001b[91mstart\u001b[0m (training logs = off)\n",
      "\t> Training with none: \u001b[91mstart\u001b[0m\n",
      "\t> Training with none: \u001b[92mdone\u001b[0m.\n",
      "\t> Training with l1: \u001b[91mstart\u001b[0m\n",
      "\t> Training with l1: \u001b[92mdone\u001b[0m.\n",
      "\t> Training with l2: \u001b[91mstart\u001b[0m\n",
      "\t> Training with l2: \u001b[92mdone\u001b[0m.\n",
      "\t> Training with dropout: \u001b[91mstart\u001b[0m\n",
      "\t> Training with dropout: \u001b[92mdone\u001b[0m.\n",
      "\t> Training with bn: \u001b[91mstart\u001b[0m\n",
      "\t> Training with bn: \u001b[92mdone\u001b[0m.\n",
      "\t> Training with l1+dropout: \u001b[91mstart\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py:1028 __call__\n        with tf.name_scope(name_scope):\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:6729 __enter__\n        scope_name = scope.__enter__()\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/contextlib.py:81 __enter__\n        return next(self.gen)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:4279 name_scope\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\n\n    ValueError: 'lenet_l1+dropout/' is not a valid scope name\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-eb9ad824ae62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     history = model.fit(x_train, y_train,\n\u001b[1;32m     18\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                         verbose=0)\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mhistory_per_instance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     print('\\t> Training with {0}: {1}done{2}.'.format(\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py:1028 __call__\n        with tf.name_scope(name_scope):\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:6729 __enter__\n        scope_name = scope.__enter__()\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/contextlib.py:81 __enter__\n        return next(self.gen)\n    /Users/theone/anaconda3/envs/learn-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:4279 name_scope\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\n\n    ValueError: 'lenet_l1+dropout/' is not a valid scope name\n"
     ]
    }
   ],
   "source": [
    "history_per_instance = dict()\n",
    "\n",
    "print(\"Experiment: {0}start{1} (training logs = off)\".format(\n",
    "    log_begin_red, log_end_format))\n",
    "for config_name in configurations:\n",
    "    # Resetting the seeds (for random number generation), to reduce the impact of randomness on the comparison:\n",
    "    tf.random.set_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    # Creating the model:\n",
    "    model = lenet(\"lenet_{}\".format(config_name),\n",
    "                  **configurations[config_name])\n",
    "    model.compile(optimizer='sgd',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Launching the training (we set `verbose=0`, so the training won't generate any logs):\n",
    "    print(\"\\t> Training with {0}: {1}start{2}\".format(\n",
    "        config_name, log_begin_red, log_end_format))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=32, epochs=300, validation_data=(x_test, y_test),\n",
    "                        verbose=0)\n",
    "    history_per_instance[config_name] = history\n",
    "    print('\\t> Training with {0}: {1}done{2}.'.format(\n",
    "        config_name, log_begin_green, log_end_format))\n",
    "print(\"Experiment: {0}done{1}\".format(log_begin_green, log_end_format))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83e29bc4780094a1020ea993be7b4ac962c5dfd5f12c392e72ec0e6eface9c7b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('learn-tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
