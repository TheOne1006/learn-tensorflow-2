{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现第一个cnn\n",
    "\n",
    "实现一个 0 - 9 的手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据\n",
    "\n",
    "如第2章所示，我们使用Tensorflow和Keras助手加载常用的[MNIST](http://yann.lecun.com/exdb/mnist)续联和测试数据集。我们还对图像进行规范化（将像素值从“[0,255]”设置为“[0,1]”，并对其进行适当的重塑（因为Tensorflow将其存储为列向量）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出类别\n",
    "num_classes = 10\n",
    "# 图片长、宽、通道\n",
    "img_rows, img_cols, img_ch = 28, 28, 1\n",
    "# 输出形状\n",
    "input_shape = (img_rows, img_cols, img_ch)\n",
    "\n",
    "# 加载数据\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# 归一化，将 0-255 => 0 - 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], *input_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建和训练 LeNet-5\n",
    "\n",
    "我们已经展示了如何根据不同的需求，以不同的方式实现CNN。在本例中，我们将使用Keras API再次展示它使实现和使用神经网络变得多么简单。\n",
    "\n",
    "### 实例化卷积层\n",
    "\n",
    "在上一联系中，我们介绍了如何对图像执行卷积。然而，在神经网络中，我们希望卷积滤波器是***可训练的***，我们可能希望在结果中添加***偏差***，并应用***激活函数***。\n",
    "\n",
    "因此，我们需要将卷积运算包装成一个“Layer”对象，类似于我们在第1章中实现的完全连接层是如何围绕矩阵运算构建的。\n",
    "\n",
    "TensorFlow 2/Keras提供了自己的tf。克拉斯。我们可以扩展的`tf.keras.Layer`。下面我们将演示如何以这种方式定义简单的卷积层：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvolutionLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_kernels=32, kernel_size=(3, 3), strides=(1, 1), use_bias=True):\n",
    "        \"\"\"\n",
    "        初始化 layer.\n",
    "        :param num_kernels:     卷积核数量\n",
    "        :param kernel_size:     核尺寸 (H x W)\n",
    "        :param strides:         垂直和水平的步长列表\n",
    "        :param use_bias:        偏差b after covolution / 激活前\n",
    "        \"\"\"\n",
    "        # First, we have to call the `Layer` super __init__(), as it initializes hidden mechanisms:\n",
    "        super().__init__()\n",
    "        # Then we assign the parameters:\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        构建 layer, 根据输入形状初始化其参数.\n",
    "        不过，第一次使用该层时，将在内部调用该函数，它也可以手动调用.\n",
    "        :param input_shape: 输入图层将接收的形状(e.g. B x H x W x C)\n",
    "        \"\"\"\n",
    "        #  获取 通道数量:\n",
    "        num_input_channels = input_shape[-1]  # assuming shape format BHWC\n",
    "\n",
    "        # 重新调整核的形状\n",
    "        kernels_shape = (*self.kernel_size,\n",
    "                         num_input_channels, self.num_kernels)\n",
    "\n",
    "        # 本案例中, 我们使用从Glorot分布中选取的值初始化过滤器:\n",
    "        glorot_uni_initializer = tf.initializers.GlorotUniform()\n",
    "        self.kernels = self.add_weight(name='kernels',\n",
    "                                       shape=kernels_shape,\n",
    "                                       initializer=glorot_uni_initializer,\n",
    "                                       trainable=True)  # 可训练的变量.\n",
    "\n",
    "        if self.use_bias:  # 如果需要偏移量，则也需要初始化:\n",
    "            self.bias = self.add_weight(name='bias',\n",
    "                                        shape=(self.num_kernels,),\n",
    "                                        # e.g., using normal distribution.\n",
    "                                        initializer='random_normal',\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        调用层并对输入张量执行其操作\n",
    "        :param inputs:  Input tensor\n",
    "        :return:        Output tensor\n",
    "        \"\"\"\n",
    "        # 进行卷积操作:\n",
    "        z = tf.nn.conv2d(inputs, self.kernels, strides=[\n",
    "                         1, *self.strides, 1], padding='VALID')\n",
    "\n",
    "        if self.use_bias:  # 如果设置偏移量:\n",
    "            z = z + self.bias\n",
    "        # 最终我们使用激活函数 (e.g. ReLU):\n",
    "        return tf.nn.relu(z)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        辅助函数返回定义的层和参数信息.\n",
    "        :return:        Dictionary containing the layer's configuration\n",
    "        \"\"\"\n",
    "        return {'num_kernels': self.num_kernels,\n",
    "                'kernel_size': self.kernel_size,\n",
    "                'strides': self.strides,\n",
    "                'use_bias': self.use_bias}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数TensorFlow数学运算（例如在 `tf.maths` 和 `tf.nn`）已经有了框架定义的导数。\n",
    "\n",
    "因此，只要一个层由这样的操作组成，**我们就不必手动定义它的反向传播**。TensorFlow将自动覆盖这一点，这节省了很多精力！\n",
    "\n",
    "因此，我们刚刚实现的卷积层是完全可操作的，并且可以在CNN中使用，我们将马上演示  \n",
    "\n",
    "***注：**由于卷积层是CNN最基本的组成部分，TensorFlow显然提供了自己的`tf.keras.layers.Conv2D`类。\n",
    "`tf.keras.layers`模块包含大量预先实现的标准层，我们建议尽可能使用（因为它们有更多高级接口和优化的操作）。\n",
    "为了演示，在本笔记本的其余部分，我们仍将使用我们自己更简单的“SimpleRevolutionLayer”，同时使用其他Keras预定义层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现LeNet-5\n",
    "\n",
    "是一个7层的CNN网络（2个卷积层，2个 max 池化层 ，3 个全连接层 + 1个 在 全连接层前整合 feature map的 展开层). \n",
    "\n",
    "以下，我们将 扩展 `tf.keras.Model`类，用于自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        初始化模型.\n",
    "        :param num_classes:    要从中预测的类数\n",
    "        \"\"\"\n",
    "        super(LeNet5, self).__init__()\n",
    "        # # 我们实例化了组成LeNet-5的各个层:\n",
    "        # self.conv1 = SimpleConvolutionLayer(6, kernel_size=(5, 5))\n",
    "        # self.conv2 = SimpleConvolutionLayer(16, kernel_size=(5, 5))\n",
    "        # ... or using the existing and (recommended) Conv2D class:\n",
    "        self.conv1 = Conv2D(6, kernel_size=(\n",
    "            5, 5), padding='same', activation='relu')\n",
    "        self.conv2 = Conv2D(16, kernel_size=(5, 5), activation='relu')\n",
    "        self.max_pool = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(120, activation='relu')\n",
    "        self.dense2 = Dense(84, activation='relu')\n",
    "        self.dense3 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        调用层并对输入张量执行操作\n",
    "        :param inputs:  Input tensor\n",
    "        :return:        Output tensor\n",
    "        \"\"\"\n",
    "        x = self.max_pool(self.conv1(inputs))        # 1st block\n",
    "        x = self.max_pool(self.conv2(x))             # 2nd block\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense3(self.dense2(self.dense1(x)))  # dense layers\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 经典 MNIST\n",
    "\n",
    "现在我们可以实例化并编译数字分类模型。为了训练它完成这项任务，我们实例化了优化器（本例中是一个简单的 _SGD_ ）并定义了损失（ _分类交叉熵_）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes)\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的模型 继承了`tf.keras.Model` 具有所有功能。例如，我们可以调用  `model.summary()` 要打印其摘要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"le_net5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  156       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 再调用 `model.summary()` 之前 ，我们必须构建它.\n",
    "# 它通常在第一次组建网络时自动完成,\n",
    "# 根据网络中的样本推断出输入形状.\n",
    "# 例如，下面的命令将构建网络(then use it for prediction):\n",
    "_ = model.predict(x_test[:10])\n",
    "\n",
    "# 但我们可以手动构建模型，提供批处理的\n",
    "# input shape ourselves:\n",
    "batched_input_shape = tf.TensorShape((None, *input_shape))\n",
    "model.build(input_shape=batched_input_shape)\n",
    "\n",
    "# Method to visualize the architecture of the network:\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在启动培训之前，我们还实例化了一些Keras回调，即在培训期间的特定点（批量培训之前/之后、完整纪元之前/之后等）自动调用的实用函数，以便对其进行监控："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # 如果验证丢失（`val_loss`）在超过3个时期内停止改善，则回调以中断培训\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    # 回调以将图表、损失和指标记录到TensorBoard中 (saving log files in `./logs` directory):\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Tensorboard 回调允许我们监控训练. 我们可以使用命令行 `tensorboard --logdir=./logs`. 打开浏览器的 [`localhost:6006`](localhost:6006) 浏览相关信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将所有内容传递给我们的模型来训练它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1875/1875 - 20s - loss: 0.4881 - accuracy: 0.8468 - val_loss: 0.1594 - val_accuracy: 0.9512\n",
      "Epoch 2/80\n",
      "1875/1875 - 18s - loss: 0.1305 - accuracy: 0.9604 - val_loss: 0.0882 - val_accuracy: 0.9729\n",
      "Epoch 3/80\n",
      "1875/1875 - 19s - loss: 0.0930 - accuracy: 0.9714 - val_loss: 0.0851 - val_accuracy: 0.9740\n",
      "Epoch 4/80\n",
      "1875/1875 - 18s - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.0633 - val_accuracy: 0.9808\n",
      "Epoch 5/80\n",
      "1875/1875 - 18s - loss: 0.0645 - accuracy: 0.9796 - val_loss: 0.0551 - val_accuracy: 0.9823\n",
      "Epoch 6/80\n",
      "1875/1875 - 17s - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.0473 - val_accuracy: 0.9841\n",
      "Epoch 7/80\n",
      "1875/1875 - 17s - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.0450 - val_accuracy: 0.9855\n",
      "Epoch 8/80\n",
      "1875/1875 - 17s - loss: 0.0458 - accuracy: 0.9858 - val_loss: 0.0407 - val_accuracy: 0.9862\n",
      "Epoch 9/80\n",
      "1875/1875 - 17s - loss: 0.0413 - accuracy: 0.9868 - val_loss: 0.0532 - val_accuracy: 0.9820\n",
      "Epoch 10/80\n",
      "1875/1875 - 17s - loss: 0.0381 - accuracy: 0.9881 - val_loss: 0.0392 - val_accuracy: 0.9867\n",
      "Epoch 11/80\n",
      "1875/1875 - 18s - loss: 0.0356 - accuracy: 0.9883 - val_loss: 0.0422 - val_accuracy: 0.9867\n",
      "Epoch 12/80\n",
      "1875/1875 - 17s - loss: 0.0323 - accuracy: 0.9904 - val_loss: 0.0338 - val_accuracy: 0.9893\n",
      "Epoch 13/80\n",
      "1875/1875 - 17s - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.0368 - val_accuracy: 0.9890\n",
      "Epoch 14/80\n",
      "1875/1875 - 17s - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.0367 - val_accuracy: 0.9878\n",
      "Epoch 15/80\n",
      "1875/1875 - 17s - loss: 0.0258 - accuracy: 0.9920 - val_loss: 0.0392 - val_accuracy: 0.9872\n",
      "Epoch 16/80\n",
      "1875/1875 - 17s - loss: 0.0241 - accuracy: 0.9926 - val_loss: 0.0342 - val_accuracy: 0.9880\n",
      "Epoch 17/80\n",
      "1875/1875 - 17s - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.0307 - val_accuracy: 0.9895\n",
      "Epoch 18/80\n",
      "1875/1875 - 17s - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.0325 - val_accuracy: 0.9898\n",
      "Epoch 19/80\n",
      "1875/1875 - 17s - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0349 - val_accuracy: 0.9883\n",
      "Epoch 20/80\n",
      "1875/1875 - 18s - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0360 - val_accuracy: 0.9877\n",
      "Epoch 21/80\n",
      "1875/1875 - 18s - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.0355 - val_accuracy: 0.9879\n",
      "Epoch 22/80\n",
      "1875/1875 - 18s - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0304 - val_accuracy: 0.9901\n",
      "Epoch 23/80\n",
      "1875/1875 - 18s - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9881\n",
      "Epoch 24/80\n",
      "1875/1875 - 19s - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0352 - val_accuracy: 0.9889\n",
      "Epoch 25/80\n",
      "1875/1875 - 19s - loss: 0.0115 - accuracy: 0.9967 - val_loss: 0.0378 - val_accuracy: 0.9879\n",
      "Epoch 26/80\n",
      "1875/1875 - 18s - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0334 - val_accuracy: 0.9893\n",
      "Epoch 27/80\n",
      "1875/1875 - 21s - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.0337 - val_accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=32, epochs=80, validation_data=(x_test, y_test),\n",
    "                    verbose=2,  # change to `verbose=1` to get a progress bar\n",
    "                                # (we opt for `verbose=2` here to reduce the log size)\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# 1875不是样本个数，而是steps。\n",
    "# steps跟batch_size有关，model.fit中的参数model.fit默认值是32，所以steps = 60000 / 32 = 1875。\n",
    "# 可以自己设置batch_size，例如设置batch_size = 1，则steps = 60000。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83e29bc4780094a1020ea993be7b4ac962c5dfd5f12c392e72ec0e6eface9c7b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('learn-tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
